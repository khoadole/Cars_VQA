{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CarInfo:\n",
    "\tbrand: str\n",
    "\tname: str\n",
    "\tprice: Optional[str]\n",
    "\tspecifications: Dict[str, str]\n",
    "\timages: List[str]\n",
    "\tid : int\n",
    "\t# description: Optional[str]\n",
    "\n",
    "class BaseScraper(ABC):\n",
    "\tnumberOfCars = -2\n",
    "\tidState = False\n",
    "\tdef __init__(self, base_url: str):\n",
    "\t\tself.base_url = base_url\n",
    "\t\tself.session = requests.Session()\n",
    "\t\tself.headers = {\n",
    "\t\t\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "\t\t}\n",
    "\t\n",
    "\tdef get_soup(self, url: str) -> BeautifulSoup:\n",
    "\t\tresponse = self.session.get(url, headers=self.headers)\n",
    "\t\treturn BeautifulSoup(response.content, 'html.parser')\n",
    "\t\n",
    "\t# @classmethod\n",
    "\t# def numberOfCarsDef(cls):\n",
    "\t# \treturn cls.numberOfCars\n",
    "\t\n",
    "\t# @classmethod\n",
    "\t# def add_car(cls):\n",
    "\t# \tcls.numberOfCars += 1\n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_next_id(cls):\n",
    "\t\t# current_id = cls.numberOfCars\n",
    "\t\tcls.numberOfCars += 1\n",
    "\t\treturn cls.numberOfCars\n",
    "\t\t# return current_id\n",
    "\t\n",
    "\t@classmethod\n",
    "\tdef get_total_cars(cls) -> int:\n",
    "\t\treturn cls.numberOfCars\n",
    "\t\n",
    "\t@classmethod \n",
    "\tdef set_numberOfCar(cls, value):\n",
    "\t\tcls.numberOfCars = value\n",
    "\n",
    "\t@classmethod\n",
    "\tdef check_state_id(cls) -> bool:\n",
    "\t\treturn cls.idState\n",
    "\t\n",
    "\t@classmethod\n",
    "\tdef pass_ID(cls) :\n",
    "\t\tcls.idState = True\n",
    "\n",
    "\t@abstractmethod\n",
    "\tdef extract_car_info(self, url: str) -> CarInfo:\n",
    "\t\tpass\n",
    "\t\n",
    "\t@abstractmethod\n",
    "\tdef get_all_car_urls(self) -> Dict[str, str]:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toyota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl specific page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website1Scraper(BaseScraper):\n",
    "\tdef extract_car_info(self, url: str) -> CarInfo:\n",
    "\t\tsoup = self.get_soup(url)\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\t# car name\n",
    "\t\t\tname = soup.select_one('.text-title.mb-32.text-left').text.strip()\n",
    "\t\t\t\n",
    "\t\t\t# price\n",
    "\t\t\tprice_element = soup.select_one('.col-7 .concept-car-info .concept-car-value')\n",
    "\n",
    "\t\t\tprice = None\n",
    "\t\t\tif price_element:\n",
    "\t\t\t\tprice_text = price_element.text.strip()\n",
    "\t\t\t\tcurrency_element = price_element.select_one('.concept-car-value-sub')\n",
    "\t\t\t\tcurrency = currency_element.text.strip() if currency_element else 'VND'\n",
    "\t\t\t\tif currency in price_text:\n",
    "\t\t\t\t\tprice_text = price_text.replace(currency, '').strip()\n",
    "\t\t\t\tprice = f\"{price_text} {currency}\"\n",
    "\t\t\t\n",
    "\t\t\t# car info\n",
    "\t\t\tspecs = {}\n",
    "\t\t\tspecs_rows = soup.select('.concept-car-info')\n",
    "\n",
    "\t\t\tfor row in specs_rows:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tkey = row.select_one('.concept-car-name').text.strip()\n",
    "\t\t\t\t\tvalue = row.select_one('.concept-car-value').text.strip()\n",
    "\t\t\t\t\tif \"Giá từ\" not in key and \"VNĐ\" not in value:\n",
    "\t\t\t\t\t\tspecs[key] = value\n",
    "\t\t\t\t\t# specs[key] = value\n",
    "\t\t\t\texcept (AttributeError, IndexError):\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# car images\n",
    "\t\t\timages = []\n",
    "\t\t\timage_elements = soup.select('.product-detail-img img') \n",
    "\t\t\t# for img in image_elements:\n",
    "\t\t\t# \tprint(img)\n",
    "\t\t\tfor img in image_elements:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tsrc = img.get('data-src')  # src\n",
    "\t\t\t\t\tif src:\n",
    "\t\t\t\t\t\tif src.startswith('//'):\n",
    "\t\t\t\t\t\t\tsrc = 'https:' + src\n",
    "\t\t\t\t\t\telif not src.startswith('http'):\n",
    "\t\t\t\t\t\t\tsrc = f\"{self.base_url.rstrip('/')}/{src.lstrip('/')}\"\n",
    "\t\t\t\t\t\timages.append(src)\n",
    "\t\t\t\texcept AttributeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# car description\n",
    "\t\t\tdescription_element = soup.select_one('.product-detail-info .product-detail-text')\n",
    "\t\t\tdescription = description_element.text.strip() if description_element else None\n",
    "\t\t\t\n",
    "\t\t\treturn CarInfo(\n",
    "\t\t\t\tname=name,\n",
    "\t\t\t\tprice=price,\n",
    "\t\t\t\tspecifications=specs,\n",
    "\t\t\t\timages=images,\n",
    "\t\t\t\tdescription=description\n",
    "\t\t\t)\n",
    "\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tlogging.error(f\"Error extracting car info from {url}: {str(e)}\")\n",
    "\t\t\traise\n",
    "\t\n",
    "\t# def get_all_car_urls(self) -> List[str]:\n",
    "\t# \treturn [\n",
    "\t# \t\t\"https://www.toyota.com.vn/camry-ce\"\n",
    "\t# \t]\n",
    "\t\n",
    "\tdef get_all_car_urls(self) -> List[str]:\n",
    "\t\tdriver = webdriver.Edge()\n",
    "\t\tdriver.get(self.base_url)\n",
    "\n",
    "\t\t# find all tabs categories\n",
    "\t\ttabs = driver.find_elements(\"css selector\", '.discovery-vehicles-tab-item')\n",
    "\t\turls = []\n",
    "\t\t# traversals all tabs\n",
    "\t\tfor tab in tabs:\n",
    "\t\t\ttab.click()  \n",
    "\t\t\thtml = driver.page_source\n",
    "\t\t\tsoup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\t\t\tcar_links = soup.select('.swiper-discovery-vehicles-item a')\n",
    "\t\t\tfor link in car_links:\n",
    "\t\t\t\thref = link.get('href')\n",
    "\t\t\t\tif href:  \n",
    "\t\t\t\t\tfull_url = self.base_url.rstrip('/') + href\n",
    "\t\t\t\t\turls.append(full_url)\n",
    "\n",
    "\t\tdriver.quit()\n",
    "\t\t# urls = list(set(urls)) # unique url\n",
    "\n",
    "\t\tprint(urls)\n",
    "\t\treturn urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cars.com\n",
    "- /shopping <br>\n",
    "- cars used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load checkpoint and setup function for crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website2Scraper(BaseScraper):\n",
    "\tcheckpoint_file = \"checkpoint.json\"\n",
    "\tdef save_to_json(self, filename: str, cars: List[CarInfo]):\n",
    "\t\tif os.path.exists(filename):\n",
    "\t\t\twith open(filename, 'r', encoding='utf-8') as f:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\texisting_data = json.load(f)\n",
    "\t\t\t\texcept json.JSONDecodeError:\n",
    "\t\t\t\t\texisting_data = []  \n",
    "\t\telse:\n",
    "\t\t\texisting_data = []\n",
    "\n",
    "\t\tcar_dicts = [vars(car) for car in cars]\n",
    "\t\texisting_data.extend(car_dicts)\n",
    "\n",
    "\t\twith open(filename, 'w', encoding='utf-8') as f:\n",
    "\t\t\tjson.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\tdef save_checkpoint(self, data):\n",
    "\t\twith open(self.checkpoint_file, \"w\") as f:\n",
    "\t\t\tjson.dump(data, f, indent=4)\n",
    "\n",
    "\tdef load_checkpoint(self):\n",
    "\t\tif os.path.exists(self.checkpoint_file):\n",
    "\t\t\twith open(self.checkpoint_file, \"r\") as f:\n",
    "\t\t\t\treturn json.load(f)\n",
    "\t\treturn {}\n",
    "\tdef get_max_id_from_checkpoint(self):\n",
    "\t\tcheckpoint = self.load_checkpoint()\n",
    "\t\tmax_id = 0\n",
    "\t\tfor brand_data in checkpoint.values():\n",
    "\t\t\tbrand_id = brand_data.get(\"id\", 0)\n",
    "\t\t\tmax_id = max(max_id, brand_id)\n",
    "\t\treturn max_id\n",
    "\t\n",
    "\tdef get_html_with_requests(self, url: str) -> str:\n",
    "\t\tresponse = self.session.get(url, headers=self.headers)\n",
    "\t\treturn response.text\n",
    "\n",
    "\t\t\n",
    "\tdef extract_car_info(self, url: str, checkpoint: dict, car_brand: str) -> CarInfo:\n",
    "\t\tsoup = self.get_soup(url)\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\t# car name\n",
    "\t\t\tname = soup.select_one('.title-section .listing-title').text.strip()\n",
    "\t\t\t\n",
    "\t\t\t# price\n",
    "\t\t\tprice = None\n",
    "\t\t\tprice_element = soup.select_one('span[data-qa=\"primary-price\"]')\n",
    "\t\t\tif price_element:\n",
    "\t\t\t\tprice = price_element.text.strip()\n",
    "\t\t\t\n",
    "\t\t\t# car info\n",
    "\t\t\tspecs = {}\n",
    "\t\t\tspecs_list = soup.select('dl.fancy-description-list dt, dl.fancy-description-list dd')\n",
    "\t\t\t\n",
    "\t\t\tcurrent_key = None\n",
    "\t\t\tfor element in specs_list:\n",
    "\t\t\t\tif element.name == 'dt':\n",
    "\t\t\t\t\tcurrent_key = element.text.strip()\n",
    "\t\t\t\telif element.name == 'dd' and current_key:\n",
    "\t\t\t\t\tvalue = element.text.strip()\n",
    "\t\t\t\t\tspecs[current_key] = value\n",
    "\t\t\t\t\tcurrent_key = None\n",
    "\t\t\t\n",
    "\t\t\t# car images\n",
    "\t\t\timages = []\n",
    "\t\t\tgallery_images = (\n",
    "\t\t\t\tsoup.select('.vdp-gallery img[modal-src]') or \n",
    "\t\t\t\tsoup.select('img[modal-src]') or  \n",
    "\t\t\t\tsoup.select('img.row-pic') \n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\tfor img in gallery_images:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tsrc = img.get('modal-src')\n",
    "\t\t\t\t\tif not src:\n",
    "\t\t\t\t\t\tsrc = img.get('src')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tif src:\n",
    "\t\t\t\t\t\tif src.startswith('//'):\n",
    "\t\t\t\t\t\t\tsrc = 'https:' + src\n",
    "\t\t\t\t\t\telif not src.startswith('http'):\n",
    "\t\t\t\t\t\t\tsrc = f\"{self.base_url.rstrip('/')}/{src.lstrip('/')}\"\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif src not in images:\n",
    "\t\t\t\t\t\t\timages.append(src)\n",
    "\t\t\t\texcept AttributeError:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t\t# ID\n",
    "\t\t\tcheckpoint = self.load_checkpoint()\n",
    "\t\t\t\n",
    "\t\t\tif(self.check_state_id()):\n",
    "\t\t\t\tcar_id = self.get_next_id()\n",
    "\t\t\telse : \n",
    "\t\t\t\tcar_id = checkpoint.get(car_brand, {}).get(\"id\", self.get_next_id()) + 1 \n",
    "\t\t\t\tself.set_numberOfCar(car_id)\n",
    "\t\t\t\t# print(f\"numberofcars : {self.numberOfCars}\")\n",
    "\t\t\t\tself.pass_ID()\n",
    "\t\t\t\t\n",
    "\t\t\treturn CarInfo(\n",
    "\t\t\t\tbrand=\"Unknown\",\n",
    "\t\t\t\tname=name,\n",
    "\t\t\t\tprice=price,\n",
    "\t\t\t\tspecifications=specs,\n",
    "\t\t\t\timages=images,\n",
    "\t\t\t\tid = car_id\n",
    "\t\t\t\t# description=description\n",
    "\t\t\t)\n",
    "\t\t\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tlogging.error(f\"Error extracting car info from {url}: {str(e)}\")\n",
    "\t\t\traise\n",
    "\n",
    "\tdef get_all_car_urls(self) -> List[Dict[str, str]]:\n",
    "\t\tall_car_urls = []\n",
    "\t\tcheckpoint = self.load_checkpoint()\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\thtml_content = self.get_html_with_requests(self.base_url)\n",
    "\n",
    "\t\t\tsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\t\t\t\n",
    "\t\t\tcar_brands_section = soup.find(\"div\", class_=\"sds-link-pack\")\n",
    "\t\t\tif not car_brands_section:\n",
    "\t\t\t\tprint(\"None Car brands\")\n",
    "\t\t\t\treturn all_car_urls\n",
    "\n",
    "\t\t\tcar_links = car_brands_section.find_all(\"a\")\n",
    "\t\t\tcar_brand_urls = [link.get(\"href\") for link in car_links if link.get(\"href\")]\n",
    "\n",
    "\t\t\tfor brand_url in car_brand_urls:\n",
    "\t\t\t\tcar_brand = brand_url.split('/')[2]\n",
    "\t\t\t\tfull_brand_url = urljoin(self.base_url, brand_url)\n",
    "\n",
    "\t\t\t\tif checkpoint:\n",
    "\t\t\t\t\tmax_id = self.get_max_id_from_checkpoint()\n",
    "\t\t\t\t\tself.set_numberOfCar(max_id)\n",
    "\t\t\t\t# checkpoint\n",
    "\t\t\t\tif car_brand in checkpoint:\n",
    "\t\t\t\t\tprint(f\"Continue brand: {car_brand}\")\n",
    "\t\t\t\t\tcurrent_url = checkpoint[car_brand][\"current_url\"]\n",
    "\t\t\t\t\tcollected_urls = checkpoint[car_brand][\"collected_urls\"]\n",
    "\t\t\t\t\tcar_id = checkpoint[car_brand].get(\"id\", self.get_next_id()) + 1 \n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"New brand: {car_brand}\")\n",
    "\t\t\t\t\tcurrent_url = full_brand_url\n",
    "\t\t\t\t\tcollected_urls = []\n",
    "\t\t\t\t\tcar_id = self.get_next_id()\n",
    "\n",
    "\t\t\t\tskip_urls = set(collected_urls)\n",
    "\n",
    "\t\t\t\twhile current_url:\n",
    "\t\t\t\t\tif current_url in skip_urls:\n",
    "\t\t\t\t\t\tprint(f\"Page done: {current_url}, skip!\")\n",
    "\t\t\t\t\t\tnext_link = soup.find(\"link\", rel=\"next\")\n",
    "\t\t\t\t\t\tif next_link and next_link.get(\"href\"):\n",
    "\t\t\t\t\t\t\tcurrent_url = urljoin(self.base_url, next_link[\"href\"])\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tcurrent_url = None \n",
    "\t\t\t\t\t\t\tprint(\"none\")\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\ttry : \n",
    "\t\t\t\t\t\t\thtml_content = self.get_html_with_requests(current_url)\n",
    "\t\t\t\t\t\t\tsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "\t\t\t\t\t\t\tvehicle_links = soup.find_all(\"a\", class_=\"vehicle-card-link\")\n",
    "\t\t\t\t\t\t\tcar_urls = [urljoin(self.base_url, link.get(\"href\")) for link in vehicle_links if link.get(\"href\")]\n",
    "\n",
    "\t\t\t\t\t\t\tcar_brand = brand_url.split('/')[2]  \n",
    "\t\t\t\t\t\t\tall_cars = []\n",
    "\t\t\t\t\t\t\tfor url in car_urls:\n",
    "\t\t\t\t\t\t\t\tcar_info = self.extract_car_info(url, checkpoint, car_brand)\n",
    "\t\t\t\t\t\t\t\tcar_info.brand = car_brand\n",
    "\t\t\t\t\t\t\t\tprint(car_info.id) \n",
    "\t\t\t\t\t\t\t\tcar_id = car_info.id\n",
    "\t\t\t\t\t\t\t\tall_cars.append(car_info)\n",
    "\t\t\t\t\t\t\t\tall_car_urls.append({\"brand\": car_brand, \"url\": url})\n",
    "\t\t\t\t\t\t\t# print(f\"carbrand: {all_car_urls}\")\n",
    "\t\t\t\t\t\t\tsavePath = \"dataset/cars_used.json\"\n",
    "\t\t\t\t\t\t\tself.save_to_json(savePath, all_cars)\n",
    "\t\t\t\t\t\t\tprint(f\"Save.\")\n",
    "\t\t\t\t\t\t\t# collected_urls.append(full_brand_url)\n",
    "\t\t\t\t\t\t\tif current_url not in collected_urls:\n",
    "\t\t\t\t\t\t\t\tcollected_urls.append(current_url)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tnext_link = soup.find(\"link\", rel=\"next\")\n",
    "\t\t\t\t\t\t\tif next_link and next_link.get(\"href\"):\n",
    "\t\t\t\t\t\t\t\tnext_url = urljoin(self.base_url, next_link[\"href\"])\n",
    "\t\t\t\t\t\t\t\t# print(f\"next url : {next_url}\")\n",
    "\t\t\t\t\t\t\t\tif \"&maximum_distance=all\" not in next_url:\n",
    "\t\t\t\t\t\t\t\t\tnext_url += \"&maximum_distance=all\" \n",
    "\t\t\t\t\t\t\t\tcurrent_url = next_url\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tnext_url = None\n",
    "\t\t\t\t\t\t\t\tcurrent_url = next_url\n",
    "\n",
    "\t\t\t\t\t\t\t# prev_url = current_url\n",
    "\t\t\t\t\t\t\t# Cập nhật checkpoint\n",
    "\t\t\t\t\t\t\tcheckpoint[car_brand] = {\n",
    "\t\t\t\t\t\t\t\t\"current_url\": current_url,\n",
    "\t\t\t\t\t\t\t\t\"collected_urls\": collected_urls,\n",
    "\t\t\t\t\t\t\t\t# \"prev_url\": prev_url,\n",
    "\t\t\t\t\t\t\t\t\"id\": car_id\n",
    "\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tself.save_checkpoint(checkpoint)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\t\t\tprint(f\"Loi trang{e}\")\n",
    "\t\t\t\t\t\t\tcurrent_url = None\n",
    "\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error: {e}\")\n",
    "\n",
    "\t\treturn all_car_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataCollector:\n",
    "    def __init__(self, scrapers: List[BaseScraper]):\n",
    "        self.scrapers = scrapers\n",
    "        \n",
    "    def collect_all_data(self, savepath: str) -> List[CarInfo]:\n",
    "        all_cars = []\n",
    "        for scraper in self.scrapers:\n",
    "            try:\n",
    "                car_urls = scraper.get_all_car_urls()\n",
    "                for data in car_urls:\n",
    "                    try:\n",
    "                        car_info = scraper.extract_car_info(data[\"url\"])\n",
    "                        car_info.brand = data[\"brand\"]\n",
    "                        # self.save_to_json(savepath, car_info)\n",
    "                        all_cars.append(car_info)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error scraping car data from {data}: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error with scraper {scraper.__class__.__name__}: {str(e)}\")\n",
    "        return all_cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl data from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue brand: acura\n",
      "Continue brand: alfa_romeo\n",
      "Continue brand: am_general\n",
      "Continue brand: aston_martin\n",
      "Continue brand: audi\n",
      "Continue brand: austin_healey\n",
      "Continue brand: bentley\n",
      "Continue brand: bmw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m scrapers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m \t\u001b[38;5;66;03m# Website1Scraper('https://www.toyota.com.vn/')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \tWebsite2Scraper(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.cars.com/shopping/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m collector \u001b[38;5;241m=\u001b[39m CarDataCollector(scrapers)\n\u001b[0;32m----> 7\u001b[0m cars \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/cars_used.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# collector.save_to_json('dataset/cars_used.json', cars)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mCarDataCollector.collect_all_data\u001b[0;34m(self, savepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scraper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrapers:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         car_urls \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_car_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m car_urls:\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[12], line 226\u001b[0m, in \u001b[0;36mWebsite2Scraper.get_all_car_urls\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m all_cars \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m car_urls:\n\u001b[0;32m--> 226\u001b[0m \tcar_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_car_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcar_brand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \tcar_info\u001b[38;5;241m.\u001b[39mbrand \u001b[38;5;241m=\u001b[39m car_brand\n\u001b[1;32m    228\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(car_info\u001b[38;5;241m.\u001b[39mid) \n",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m, in \u001b[0;36mWebsite2Scraper.extract_car_info\u001b[0;34m(self, url, checkpoint, car_brand)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_car_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, checkpoint: \u001b[38;5;28mdict\u001b[39m, car_brand: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CarInfo:\n\u001b[0;32m---> 55\u001b[0m \tsoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \t\u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m \t\t\u001b[38;5;66;03m# car name\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \t\tname \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.title-section .listing-title\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mBaseScraper.get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[0;32m---> 22\u001b[0m \tresponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/textMining/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrapers = [\n",
    "\t# Website1Scraper('https://www.toyota.com.vn/')\n",
    "\tWebsite2Scraper('https://www.cars.com/shopping/')\n",
    "]\n",
    "\n",
    "collector = CarDataCollector(scrapers)\n",
    "cars = collector.collect_all_data(\"dataset/cars_used.json\")\n",
    "# collector.save_to_json('dataset/cars_used.json', cars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
